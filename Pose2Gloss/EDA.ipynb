{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>g_type</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>gloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/horang1804/HDD1/dataset/aihub_sign/004.수...</td>\n",
       "      <td>WORD</td>\n",
       "      <td>1.507</td>\n",
       "      <td>3.981</td>\n",
       "      <td>학교연혁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/horang1804/HDD1/dataset/aihub_sign/004.수...</td>\n",
       "      <td>WORD</td>\n",
       "      <td>1.489</td>\n",
       "      <td>3.117</td>\n",
       "      <td>야단치다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/horang1804/HDD1/dataset/aihub_sign/004.수...</td>\n",
       "      <td>WORD</td>\n",
       "      <td>1.605</td>\n",
       "      <td>3.337</td>\n",
       "      <td>수감자</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/horang1804/HDD1/dataset/aihub_sign/004.수...</td>\n",
       "      <td>WORD</td>\n",
       "      <td>1.704</td>\n",
       "      <td>4.152</td>\n",
       "      <td>간호사</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/horang1804/HDD1/dataset/aihub_sign/004.수...</td>\n",
       "      <td>WORD</td>\n",
       "      <td>1.187</td>\n",
       "      <td>2.783</td>\n",
       "      <td>형식</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path g_type start_time  \\\n",
       "0  /home/horang1804/HDD1/dataset/aihub_sign/004.수...   WORD      1.507   \n",
       "1  /home/horang1804/HDD1/dataset/aihub_sign/004.수...   WORD      1.489   \n",
       "2  /home/horang1804/HDD1/dataset/aihub_sign/004.수...   WORD      1.605   \n",
       "3  /home/horang1804/HDD1/dataset/aihub_sign/004.수...   WORD      1.704   \n",
       "4  /home/horang1804/HDD1/dataset/aihub_sign/004.수...   WORD      1.187   \n",
       "\n",
       "  end_time gloss  \n",
       "0    3.981  학교연혁  \n",
       "1    3.117  야단치다  \n",
       "2    3.337   수감자  \n",
       "3    4.152   간호사  \n",
       "4    2.783    형식  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./gloss_info.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'. 가다' -> '가다'\n",
      "', 나는' -> '나는'\n",
      "'? 어디로' -> '어디로'\n",
      "': 시작1' -> '시작1'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    # 정규 표현식으로 문자열 앞의 문장부호를 제거\n",
    "    cleaned_text = re.sub(r'^[^\\w가-힣]+', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "# 테스트 예시\n",
    "test_cases = ['. 가다', ', 나는', '? 어디로', ': 시작1']\n",
    "\n",
    "for case in test_cases:\n",
    "    cleaned = remove_punctuation(case)\n",
    "    print(f\"'{case}' -> '{cleaned}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'종로3가' -> ['종로3가']\n",
      "'의문1' -> ['의문1']\n",
      "'119' -> ['119']\n",
      "'8번째' -> ['팔', '번째']\n",
      "'10번째' -> ['일', '영', '번째']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def convert_number_to_korean(num_str):\n",
    "    korean_numbers = {\n",
    "        '1': '일', '2': '이', '3': '삼', '4': '사', \n",
    "        '5': '오', '6': '육', '7': '칠', '8': '팔', \n",
    "        '9': '구', '0': '영'\n",
    "    }\n",
    "    return ''.join(korean_numbers[char] for char in num_str)\n",
    "\n",
    "def process_text(text):\n",
    "    if re.match(r'^\\d+$', text):\n",
    "        return [text]  # 숫자만 있을 때 그대로 반환\n",
    "\n",
    "    if re.search(r'\\d+', text):\n",
    "        if text.isdigit():\n",
    "            return [text]  # 순수 숫자일 경우 그대로 반환\n",
    "        elif re.match(r'^\\d+번째$', text):\n",
    "            # 'N번째' 패턴 처리\n",
    "            number_part = re.match(r'(\\d+)', text).group(0)\n",
    "            return list(convert_number_to_korean(number_part)) + ['번째']\n",
    "        else:\n",
    "            return [text]  # 숫자와 다른 문자가 섞인 경우 그대로 반환\n",
    "\n",
    "    return [text]  # 다른 모든 경우 원본 그대로 반환\n",
    "\n",
    "# 테스트 예시\n",
    "test_cases = [\n",
    "    '종로3가', '의문1', '119', '8번째', '10번째'\n",
    "]\n",
    "\n",
    "for case in test_cases:\n",
    "    result = process_text(case)\n",
    "    print(f\"'{case}' -> {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'종로3가' -> '종로3가'\n",
      "'의문1' -> '의문'\n",
      "'서울역2' -> '서울역'\n",
      "'나사렛100' -> '나사렛'\n",
      "'hello123' -> 'hello'\n",
      "'test' -> 'test'\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_numbers_at_end(text):\n",
    "    # 정규 표현식: 문자 끝에 붙은 숫자를 제거\n",
    "    cleaned_text = re.sub(r'(\\D+)\\d+$', r'\\1', text)\n",
    "    return cleaned_text\n",
    "\n",
    "# 테스트 예시\n",
    "test_cases = [\n",
    "    \"종로3가\", \"의문1\", \"서울역2\", \"나사렛100\", \"hello123\", \"test\"\n",
    "]\n",
    "\n",
    "for case in test_cases:\n",
    "    result = remove_numbers_at_end(case)\n",
    "    print(f\"'{case}' -> '{result}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_list = [\"서울농아인협회\", \"광주시수어통역센터\", \"서울특별시농아인협회\", \"부산광역시농아인협회\", \n",
    "    \"광주광역시농아인협회\", \"전북농아인협회\", \"충남농아인협회\", \"경기농아인협회\", \"강릉시수어통역센터\",\n",
    "    \"고양시청\", \"서울대공원\", \"경기도청\", \"영등포구지부\", \"강남구청\", \"서초구지부\", \n",
    "    \"광산구주민지원센터\", \"서대문구수어통역센터\", \"강화군수어통역센터\", \"완주군주민지원센터\", \n",
    "    \"순천시지부\", \"안성시지부\", \"강릉시청\", \"부천시청\", \"청주\", \"김포시청\", \"김포시수어통역센터\",\n",
    "    \"울산광역시농아인협회\", \"광명시청\", \"강화군지부\", \"대구광역시농아인협회\", \"서울대학교\", \n",
    "    \"양평군수어통역센터\", \"강남구수어통역센터\", \"서울특별시수어통역센터\", \"충북도청\", \n",
    "    \"부천시수어통역센터\", \"광주광역시청\", \"포항시청\", \"의성군지부\", \"평창군수어통역센터\", \n",
    "    \"전주시청\", \"경주시청\", \"고성군주민지원센터\", \"서초구수어통역센터\", \"제주도청\", \n",
    "    \"마포구수어통역센터\", \"금천구지부\", \"양천구수어통역센터\", \"용산구지부\", \"서초구지부\", \n",
    "    \"관악구지부\", \"성남시지부\", \"성남시청\", \"성동구주민지원센터\", \"광주광역시청\", \n",
    "    \"성동구지부\", \"포항시수어통역센터\", \"제천시지부\", \"용인시청\", \"군위군지부\", \"광양군지부\",\n",
    "    \"청주시지부\", \"부안군지부\", \"성주시지부\", \"순천시청\", \"전주시지부\", \"고창군수어통역센터\",\n",
    "    \"고성군수어통역센터\", \"전북도청\", \"동대문구주민지원센터\", \"영등포구주민지원센터\", \n",
    "    \"서울특별시수어통역센터\", \"강화군수어통역센터\", \"포천시지부\", \"광양군수어통역센터\", \n",
    "    \"성북구지부\", \"용산구주민지원센터\", \"성동구수어통역센터\", \"강화군주민지원센터\", \n",
    "    \"강원도청\", \"고양시지부\", \"광산구주민지원센터\", \"서대문구주민지원센터\", \"대구광역시청\",\n",
    "    \"청주\", \"강남구청\", \"광산구주민지원센터\", \"강동구지부\", \"강원도농아인협회\", \"강원도\", \n",
    "    \"고양시청\", \"고양시수어통역센터\", \"김천시지부\", \"김해시청\", \"대덕구주민지원센터\", \n",
    "    \"대덕구지부\", \"대전광역시농아인협회\", \"부산광역시농아인협회\", \"성남시수어통역센터\",\n",
    "    \"성남시청\", \"성북구지부\", \"서울대학교\", \"서울특별시농아인협회\", \"성남시청\", \"순창군지부\",\n",
    "    \"안양시청\", \"전북농아인협회\", \"전주시청\", \"중랑구주민지원센터\", \"창원시청\", \n",
    "    \"창원시수화통역센터\", \"창녕군수화통역센터\", \"창원시지부\", \"춘천시청\", \"춘천시수어통역센터\",\n",
    "    \"청주\", \"춘천시수어통역센터\", \"충북농아인협회\", \"태안군수어통역센터\", \"태안군지부\",\n",
    "    \"포항시청\", \"포항시수어통역센터\", \"평택시청\", \"평택시수어통역센터\", \"홍성군지부\",\n",
    "    \"강릉시청\", \"구로구지부\", \"영월군수어통역센터\", \"대전광역시청\", \"구미시청\", \n",
    "    \"군포시청\", \"군포시수어통역센터\", \"금천구청\", \"금정구지부\", \"나주시지부\", \n",
    "    \"동두천시수어통역센터\", \"동대문구주민지원센터\", \"마포구수어통역센터\", \"보성군주민지원센터\",\n",
    "    \"부산광역시청\", \"서울특별시수어통역센터\", \"성동구지부\", \"성남시수어통역센터\", \n",
    "    \"성남시청\", \"성북구수어통역센터\", \"성북구지부\", \"성주시청\", \"성주시수어통역센터\",\n",
    "    \"수원시청\", \"수원시수어통역센터\", \"순창군지부\", \"안양시청\", \"양천구지부\", \n",
    "    \"영광군수어통역센터\", \"영광군지부\", \"영덕군지부\", \"영동군수어통역센터\", \"영주시지부\",\n",
    "    \"영천시청\", \"예천군지부\", \"옥천군지부\", \"울산광역시청\", \"울산광역시농아인협회\",\n",
    "    \"울진군수어통역센터\", \"원주시수어통역센터\", \"원주시청\", \"울진군지부\", \"이천시청\", \n",
    "    \"이천시수어통역센터\", \"인천광역시청\", \"인천광역시농아인협회\", \"장성군주민지원센터\",\n",
    "    \"장성군수어통역센터\", \"장수군수어통역센터\", \"전라북도청\", \"전남도청\", \"전주시청\",\n",
    "    \"전주시수어통역센터\", \"정읍시청\", \"제주시청\", \"제주특별자치도농아인협회\", \n",
    "    \"조치원수어통역센터\", \"진도군수어통역센터\", \"진도군지부\", \"진주광역시청\", \n",
    "    \"진주시청\", \"진주시수어통역센터\", \"진천군지부\", \"진천군수어통역센터\", \"청주시청\",\n",
    "    \"청주시수어통역센터\", \"춘천시청\", \"충주시수어통역센터\", \"충주시청\", \"충청남도청\",\n",
    "    \"태안군지부\", \"통영시청\", \"포천시청\", \"포항시청\", \"포항시수어통역센터\", \n",
    "    \"하동군수화통역센터\", \"하남시청\", \"하남시수어통역센터\", \"합천군수어통역센터\", \n",
    "    \"함안군지부\", \"함안군수화통역센터\", \"함양군수어통역센터\", \"해남군지부\", \n",
    "    \"해남군수어통역센터\", \"홍성군수어통역센터\", \"화성시청\", \"화성시수어통역센터\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4378 266\n",
      "유턴 240\n",
      "맞다 2668\n",
      "돈 413\n",
      "에어컨 736\n",
      "켜다 352\n",
      "좌회전 414\n",
      "우회전 443\n",
      "확인 438\n",
      "여기 1260\n",
      "내리다 5617\n",
      "버스 1311\n",
      "곳 8598\n",
      "기차 379\n",
      "차내리다 5086\n",
      "난방 272\n",
      "꺼지다 96\n",
      "끄다 288\n",
      "명동 442\n",
      "가다 3491\n",
      "걷다 87\n",
      "알다 81\n",
      "카드 1035\n",
      "되다 356\n",
      "신호등 555\n",
      "다음 6392\n",
      "도착 4073\n",
      "병원 375\n",
      "사거리 426\n",
      "대로 320\n",
      "편지 320\n",
      "저기 3881\n",
      "지하철 1767\n",
      "청음회관 336\n",
      "터널 320\n",
      "일 386\n",
      "호 528\n",
      "은행 446\n",
      "시청 368\n",
      "오케이 672\n",
      "언덕 352\n",
      "경찰 384\n",
      "샛길 582\n",
      "군청 319\n",
      "건너다 433\n",
      "얕보다 64\n",
      "보건소 336\n",
      "지금 223\n",
      "따뜻하다 144\n",
      "근근이 64\n",
      "결점 64\n",
      "유구무언 64\n",
      "함구 64\n",
      "막히다 64\n",
      "급하다 97\n",
      "뻔뻔 63\n",
      "좋다 63\n",
      "영수증 240\n",
      "받다 98\n",
      "가능 317\n",
      "육교 320\n",
      "결심 62\n",
      "없다 795\n",
      "춥다 113\n",
      "지도 77\n",
      "유리 180\n",
      "올리다 160\n",
      "지름길 377\n",
      "격노 49\n",
      "빨리 269\n",
      "놀랍다 48\n",
      "내뱉다 48\n",
      "필요 128\n",
      "당신 48\n",
      "판박이 48\n",
      "순식간 48\n",
      "대출 48\n",
      "주다 52\n",
      "지불하다 48\n",
      "맥없다 48\n",
      "왼쪽 52\n",
      "오른쪽 50\n",
      "화나다 48\n",
      "포기 48\n",
      "아부 48\n",
      "모르다 48\n",
      "백화점 320\n",
      "보다 1060\n",
      "봐주다 48\n",
      "욕하다 48\n",
      "괜찮다 128\n",
      "빌리다 48\n",
      "무자비 48\n",
      "아하 48\n",
      "돕다 48\n",
      "의문 47\n",
      "무례 48\n",
      "폐업 47\n",
      "힘들다 47\n",
      "거만 47\n",
      "나 78\n",
      "거지 46\n",
      "사라지다 334\n",
      "손바닥찍다 300\n",
      "야단치다 45\n",
      "단말기터치 455\n",
      "다시 181\n",
      "배려 45\n",
      "응급실 150\n",
      "밉다 45\n",
      "째려보다 44\n",
      "자다 44\n",
      "초대 44\n",
      "때리다 44\n",
      "뚱뚱하다 44\n",
      "안전벨트 65\n",
      "전 1601\n",
      "알려주다 133\n",
      "공항 462\n",
      "맥도날드 320\n",
      "자판기 416\n",
      "고장 670\n",
      "죽다 368\n",
      "설명 81\n",
      "역무원 49\n",
      "부르다 81\n",
      "강요 33\n",
      "불가능 455\n",
      "목적 1215\n",
      "아직 48\n",
      "지갑 256\n",
      "실종 1740\n",
      "다르다 128\n",
      "불량 559\n",
      "접근 207\n",
      "열쇠 255\n",
      "찾다 1088\n",
      "차 319\n",
      "엉덩이 112\n",
      "트렁크열다 185\n",
      "파리바게트 304\n",
      "이마트 288\n",
      "시 95\n",
      "잃어버리다 1182\n",
      "차두다 64\n",
      "가방 302\n",
      "잘 480\n",
      "안되다 1415\n",
      "택시 560\n",
      "아이 305\n",
      "이 161\n",
      "십 286\n",
      "사용 864\n",
      "기본 64\n",
      "만원 64\n",
      "분 143\n",
      "시간 385\n",
      "물품보관 374\n",
      "영등포 303\n",
      "길 1359\n",
      "방법 1770\n",
      "있다 238\n",
      "학교 344\n",
      "스타벅스 288\n",
      "우산 256\n",
      "쓰러지다 93\n",
      "표 66\n",
      "여의도 336\n",
      "어렵다 63\n",
      "구 144\n",
      "차밀리다 160\n",
      "원하다 255\n",
      "바꾸다 57\n",
      "종로 350\n",
      "무엇 1684\n",
      "몇분 80\n",
      "기다리다 96\n",
      "천안아산역 80\n",
      "공기청정기 160\n",
      "불편하다 64\n",
      "아파트 256\n",
      "뒤 192\n",
      "들어올리다 94\n",
      "물건 175\n",
      "팔 111\n",
      "서울 75\n",
      "딱 50\n",
      "삼 336\n",
      "안 246\n",
      "검사 41\n",
      "첫차 61\n",
      "잠깐 47\n",
      "어떻게 39\n",
      "알려받다 105\n",
      "교환하다 61\n",
      "갈아타다 330\n",
      "사다 94\n",
      "또 59\n",
      "타다 60\n",
      "고속 92\n",
      "돈주다 76\n",
      "서울역 197\n",
      "송파 366\n",
      "주차 125\n",
      "꼿다 45\n",
      "오천원 45\n",
      "철로 75\n",
      "떨어지다 120\n",
      "짐 96\n",
      "나르다 60\n",
      "국립박물관 45\n",
      "고속터미널 166\n",
      "전화걸다 45\n",
      "편의점 126\n",
      "얼마 112\n",
      "지연되다 75\n",
      "밤 33\n",
      "가깝다 107\n",
      "약 145\n",
      "막차 46\n",
      "운전 48\n",
      "천천히 45\n",
      "어린이집 62\n",
      "곳곳 295\n",
      "엘리베이터 207\n",
      "뼈곳 109\n",
      "이화여대 320\n",
      "잘못 37\n",
      "의자 112\n",
      "핸드폰 208\n",
      "냄새 48\n",
      "아프다 48\n",
      "계산 128\n",
      "틀리다 64\n",
      "문 112\n",
      "번호 173\n",
      "안경 64\n",
      "회의실 80\n",
      "사진기 144\n",
      "보청기 80\n",
      "반대 33\n",
      "시계 128\n",
      "계단 154\n",
      "안내소 96\n",
      "노트북 128\n",
      "반지 128\n",
      "남다 62\n",
      "돈받다 46\n",
      "화장실 93\n",
      "만 47\n",
      "오 99\n",
      "잘못하다 59\n",
      "끝 33\n",
      "몇호 80\n",
      "천호 48\n",
      "사람 46\n",
      "분당 34\n",
      "사 51\n",
      "정기권 60\n",
      "틈 48\n",
      "도움받다 120\n",
      "문안열리다 60\n",
      "안내하다 90\n",
      "도와주다 45\n",
      "교통카드 45\n",
      "충전 59\n",
      "번째 44\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "dic = defaultdict(int)\n",
    "# for i, (k, v) in enumerate(df[df.g_type=='WORD'].gloss.value_counts().items()):\n",
    "replace_dict = {'<sos>':'<sos>',\n",
    "                '<eos>':'<eos>',\n",
    "                '<pad>':'<pad>',\n",
    "                '3.1절':'삼일절',\n",
    "                '100만원':'백|만|원',\n",
    "                '50분':'오|십|분',\n",
    "                '10만원':'십|만|원',\n",
    "                '5천원':'오|천|원',\n",
    "                '1달':'일|달',\n",
    "                '1시':'일|시',\n",
    "                '2호':'이|호',\n",
    "                '10분':'십|분',\n",
    "                '3시':'삼|시',\n",
    "                '1호':'일|호',\n",
    "                '9호':'구|호',\n",
    "                '1시간':'일|시간',\n",
    "                '12시':'십|이|시',\n",
    "                '3':'삼',\n",
    "                '1':'일',\n",
    "                '1회':'일|회',\n",
    "                '12':'십|이',\n",
    "                '5분':'오|분',\n",
    "                '3호':'삼|호',\n",
    "                '8호':'팔|호',\n",
    "                '5호':'오|호',\n",
    "                '40분':'사|십|분',\n",
    "                '8번째':'팔|번째',\n",
    "                '10번째':'십|번쩨',\n",
    "                '12번째':'십|이|번째',\n",
    "                '9번째':'9|번째',\n",
    "                '4명':'사|명',\n",
    "                '5천':'오|천',\n",
    "                '24':'이|십|사',\n",
    "                '10번쨰':'십|번쩨',\n",
    "                '9':'구'\n",
    "                }\n",
    "replace_key = list(replace_dict.keys())\n",
    "delete_list = [\n",
    "'38선',\n",
    "'완완주군수어통역센터',\n",
    "'분혹색',\n",
    "'ㅊ',\n",
    "'U',\n",
    "'119',\n",
    "'분혹색',\n",
    "'4사람',\n",
    "'나사렛',\n",
    "'을지로3가',\n",
    "'종로5가',\n",
    "'을지로4가',\n",
    "'종로3가',\n",
    "'0만원',\n",
    "'장애인기능경기대회',\n",
    "'서대문농아인복지관',\n",
    "'장애인등에대한특수교육법',\n",
    "]+region_list\n",
    "for i, (k, v) in enumerate(df.gloss.value_counts().items()):\n",
    "    gloss_list = k.split('|')\n",
    "    for g in gloss_list:\n",
    "        gloss = g\n",
    "\n",
    "        # 0. replace\n",
    "        if g in replace_key:\n",
    "            for gg in replace_dict[g].split('|'):\n",
    "                dic[gg] += v\n",
    "        else:\n",
    "            # 1. delete\n",
    "            if g in delete_list:\n",
    "                continue\n",
    "            if len(g)>2:\n",
    "                if g[-2:] == '협회': continue\n",
    "                if g[-2:] == '센터': continue\n",
    "                if g[-2:] == '시청': continue\n",
    "                if g[-2:] == '지부': continue\n",
    "\n",
    "            # 2. remove space(' ')\n",
    "            gloss = gloss.replace(' ','')\n",
    "\n",
    "            # 3. remove Enter('\\n')\n",
    "            gloss = gloss.replace('\\n','')\n",
    "            \n",
    "            # 4. remove marks('.?!,')\n",
    "            gloss = re.sub(r'^[^\\w가-힣]+', '', gloss)\n",
    "            gloss = re.sub(r'^[^\\w가-힣]+', '', gloss)\n",
    "\n",
    "            # # 5. remove end number\n",
    "            # gloss = re.sub(r'(\\D+)\\d+$', r'\\1', gloss)\n",
    "            \n",
    "            # # 6. remove 괄호\n",
    "            # gloss = re.sub(r'\\(.*?\\)', '', gloss)\n",
    "            # gloss = re.sub(r'\\s*,\\s*', ', ', gloss.strip())\n",
    "            \n",
    "\n",
    "            # # 7. split korean number\n",
    "            # pattern = r'[일이삼사오육칠팔구십백천만억조]'\n",
    "            # text = re.findall(pattern, gloss)\n",
    "            # is_number = ''.join(text) == gloss\n",
    "            # if is_number:\n",
    "            #     gloss = '|'.join(text)\n",
    "            #     for t in text:\n",
    "            #         dic[t] += v\n",
    "            #     # print(text)\n",
    "            # else:\n",
    "            #     dic[gloss]+=v\n",
    "            dic[gloss]+=v\n",
    "\n",
    "dic = {k:v for k,v in dic.items() if v >32}\n",
    "print(i+1, len(dic))\n",
    "for k, v in dic.items():\n",
    "    # if any(num in k for num in [str(i) for i in range(10)]):\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2id = {'<pad>':0,\n",
    "#            '<sos>':1,\n",
    "#            '<eos>':2}\n",
    "# for i, word in enumerate(iter(dic.keys())):\n",
    "#     word2id[word] = i+3\n",
    "\n",
    "# id2word = {v:k for k, v in word2id.items()}\n",
    "\n",
    "#################################################\n",
    "\n",
    "word2id = {}\n",
    "for i, word in enumerate(iter(dic.keys())):\n",
    "    word2id[word] = i\n",
    "\n",
    "id2word = {v:k for k, v in word2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gloss(x):\n",
    "    ret = []\n",
    "    gloss_list = str(x).split('|')\n",
    "    for g in gloss_list[1:-1]:\n",
    "        gloss = g\n",
    "\n",
    "        # 0. replace\n",
    "        if gloss in replace_key:\n",
    "            ret += replace_dict[gloss].split('|')\n",
    "                \n",
    "        else:\n",
    "            # 1. delete\n",
    "            if g in delete_list:\n",
    "                continue\n",
    "            if len(g)>2:\n",
    "                if g[-2:] == '협회': continue\n",
    "                if g[-2:] == '센터': continue\n",
    "                if g[-2:] == '시청': continue\n",
    "                if g[-2:] == '지부': continue\n",
    "\n",
    "            # 2. remove space(' ')\n",
    "            gloss = gloss.replace(' ','')\n",
    "\n",
    "            # 3. remove Enter('\\n')\n",
    "            gloss = gloss.replace('\\n','')\n",
    "            \n",
    "            # 4. remove marks('.?!,')\n",
    "            gloss = re.sub(r'^[^\\w가-힣]+', '', gloss)\n",
    "            gloss = re.sub(r'^[^\\w가-힣]+', '', gloss)\n",
    "\n",
    "            # # 5. remove end number\n",
    "            # gloss = re.sub(r'(\\D+)\\d+$', r'\\1', gloss)\n",
    "            \n",
    "            # # 6. remove 괄호\n",
    "            # gloss = re.sub(r'\\(.*?\\)', '', gloss)\n",
    "            # gloss = re.sub(r'\\s*,\\s*', ', ', gloss.strip())\n",
    "\n",
    "            # 7. split korean number\n",
    "            # pattern = r'[일이삼사오육칠팔구십백천만억조]'\n",
    "            # text = re.findall(pattern, gloss)\n",
    "            # is_number = ''.join(text) == gloss\n",
    "            # if is_number:\n",
    "            #     gloss = '|'.join(text)\n",
    "            #     for t in text:\n",
    "            #         ret.append(t)\n",
    "            #     # print(text)\n",
    "            # else:\n",
    "            #     ret.append(gloss)\n",
    "            ret.append(gloss)\n",
    "    return '|'.join(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_key = list(word2id.keys())\n",
    "dic_key.append('<sos>')\n",
    "dic_key.append('<eos>')\n",
    "def is_in_dic(x):\n",
    "    cnt = 0\n",
    "    for i, g in enumerate(x.split('|')):\n",
    "        if g in dic_key:\n",
    "            cnt+=1\n",
    "    return (i+1==cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    file_id                   gloss\n",
      "0  NIA_SL_WORD0909_REAL17_F          <sos>|특선|<eos>\n",
      "1  NIA_SL_WORD0556_REAL18_F          <sos>|방도|<eos>\n",
      "2  NIA_SL_WORD1948_REAL17_F   <sos>|군위군수어통역센터|<eos>\n",
      "3   NIA_SL_SEN0786_REAL17_F  <sos>|여기|차내리다|맞다|<eos>\n",
      "4   NIA_SL_SEN0350_REAL18_F       <sos>|고속|빨리|<eos>\n",
      "                    file_id       gloss\n",
      "0  NIA_SL_WORD0909_REAL17_F          특선\n",
      "1  NIA_SL_WORD0556_REAL18_F          방도\n",
      "2  NIA_SL_WORD1948_REAL17_F            \n",
      "3   NIA_SL_SEN0786_REAL17_F  여기|차내리다|맞다\n",
      "4   NIA_SL_SEN0350_REAL18_F       고속|빨리\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24912/2341523209.py:7: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  df2.exist = df2.gloss.apply(is_in_dic)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9998, 4223)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'gloss_label_valid_original.csv'\n",
    "save_name = 'gloss_label_valid_cl.csv'\n",
    "\n",
    "df2 = pd.read_csv(file_name)\n",
    "print(df2.head())\n",
    "df2.gloss = df2.gloss.apply(process_gloss)\n",
    "df2.exist = df2.gloss.apply(is_in_dic)\n",
    "print(df2.head())\n",
    "len(df2), df2.exist.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>gloss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NIA_SL_SEN1658_REAL10_F</td>\n",
       "      <td>병원|저기|다음|도착|내리다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NIA_SL_SEN0523_REAL16_F</td>\n",
       "      <td>학교|가다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NIA_SL_SEN1397_REAL14_F</td>\n",
       "      <td>여의도|무엇|방법</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NIA_SL_SEN0846_REAL14_F</td>\n",
       "      <td>차내리다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NIA_SL_SEN0285_REAL08_F</td>\n",
       "      <td>아이|실종|찾다|도움받다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76929</th>\n",
       "      <td>NIA_SL_SEN1072_REAL11_F</td>\n",
       "      <td>공기청정기|사용|안되다|불량</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76930</th>\n",
       "      <td>NIA_SL_SEN1105_REAL09_F</td>\n",
       "      <td>에어컨|불량|사용|안되다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76931</th>\n",
       "      <td>NIA_SL_SEN1401_REAL07_F</td>\n",
       "      <td>여의도|가다|방법</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76932</th>\n",
       "      <td>NIA_SL_SEN0585_REAL04_F</td>\n",
       "      <td>응급실|차내리다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76935</th>\n",
       "      <td>NIA_SL_SEN1019_REAL13_F</td>\n",
       "      <td>편의점|곳</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33196 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       file_id            gloss\n",
       "3      NIA_SL_SEN1658_REAL10_F  병원|저기|다음|도착|내리다\n",
       "4      NIA_SL_SEN0523_REAL16_F            학교|가다\n",
       "9      NIA_SL_SEN1397_REAL14_F        여의도|무엇|방법\n",
       "10     NIA_SL_SEN0846_REAL14_F             차내리다\n",
       "11     NIA_SL_SEN0285_REAL08_F    아이|실종|찾다|도움받다\n",
       "...                        ...              ...\n",
       "76929  NIA_SL_SEN1072_REAL11_F  공기청정기|사용|안되다|불량\n",
       "76930  NIA_SL_SEN1105_REAL09_F    에어컨|불량|사용|안되다\n",
       "76931  NIA_SL_SEN1401_REAL07_F        여의도|가다|방법\n",
       "76932  NIA_SL_SEN0585_REAL04_F         응급실|차내리다\n",
       "76935  NIA_SL_SEN1019_REAL13_F            편의점|곳\n",
       "\n",
       "[33196 rows x 2 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df2[df2.exist]\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(save_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g_list = []\n",
    "# for g in list(df[df.g_type=='SEN'].gloss):\n",
    "#     if g is not np.nan:\n",
    "#         g_candidate=g.split('|')\n",
    "#         for gc in g_candidate:\n",
    "#             g_list+=split_and_process(gc)\n",
    "\n",
    "# for g in list(df[df.g_type=='WORD'].gloss):\n",
    "#     if g is not np.nan:\n",
    "#         g_candidate=g.split('|')\n",
    "#         for gc in g_candidate:\n",
    "#             g_list+=split_and_process(gc)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('word2id.json', 'w') as json_file:\n",
    "    json.dump(word2id, json_file, ensure_ascii=False)\n",
    "with open('id2word.json', 'w') as json_file:\n",
    "    json.dump(id2word, json_file, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./word2id.json', \"r\") as st_json: word2id = json.load(st_json)\n",
    "with open('./id2word.json', \"r\") as st_json: id2word = json.load(st_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>phase</th>\n",
       "      <th>situation</th>\n",
       "      <th>detail</th>\n",
       "      <th>gloss</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NIA_SL_G4_SAFETYACCIDENTATCHILDRENSAMUSEMENTFA...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>기타재난</td>\n",
       "      <td>SAFETYACCIDENTATCHILDRENSAMUSEMENTFACILITIES</td>\n",
       "      <td>회사1|다스리다1|편하다1|단체1|날짜:8월|아이1|편하다1|실수1|사고1|계산1|...</td>\n",
       "      <td>[행정안전부] 8월 4주 어린이 안전사고 14건 발생. 시소는 서로 마주보고 앉아서...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NIA_SL_G4_SAFETYACCIDENTATCHILDRENSAMUSEMENTFA...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>기타재난</td>\n",
       "      <td>SAFETYACCIDENTATCHILDRENSAMUSEMENTFACILITIES</td>\n",
       "      <td>회사1|편하다1|날짜:0월5일|편하다1|사고1|그네2|놀다1|아니다1</td>\n",
       "      <td>[행정안전부] 최근 5일 어린이 안전사고 15건 발생. 그네를 이용할 때 움직이는 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NIA_SL_G4_SAFETYACCIDENTATCHILDRENSAMUSEMENTFA...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>기타재난</td>\n",
       "      <td>SAFETYACCIDENTATCHILDRENSAMUSEMENTFACILITIES</td>\n",
       "      <td>회사1|다스리다1|편하다1|단체1|지난1|날1|아이1|편하다1|실수1|사고1|계산1...</td>\n",
       "      <td>[행정안전부] 최근 15일 어린이 안전사고 11건 발생. 건너는 기구를 이용할 때,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NIA_SL_G4_SAFETYACCIDENTATCHILDRENSAMUSEMENTFA...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>기타재난</td>\n",
       "      <td>SAFETYACCIDENTATCHILDRENSAMUSEMENTFACILITIES</td>\n",
       "      <td>회사1|다스리다1|편하다1|단체1|날짜:4월|아이1|편하다1|실수1|사고1|계산1|...</td>\n",
       "      <td>[행정안전부] 4월 어린이 안전사고 12건 발생. 놀이시설에서는 학용품 및 공 등 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NIA_SL_G4_SAFETYACCIDENTATCHILDRENSAMUSEMENTFA...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>기타재난</td>\n",
       "      <td>SAFETYACCIDENTATCHILDRENSAMUSEMENTFACILITIES</td>\n",
       "      <td>회사1|다스리다1|편하다1|단체1|아이1|편하다1|실수1|사고1|계산1|놀다1|때1...</td>\n",
       "      <td>[행정안전부] 어제 어린이 안전사고 7건 발생. 물이용 놀이기구를 이용할 때, 놀이...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name  phase situation  \\\n",
       "0  NIA_SL_G4_SAFETYACCIDENTATCHILDRENSAMUSEMENTFA...    NaN      기타재난   \n",
       "1  NIA_SL_G4_SAFETYACCIDENTATCHILDRENSAMUSEMENTFA...    NaN      기타재난   \n",
       "2  NIA_SL_G4_SAFETYACCIDENTATCHILDRENSAMUSEMENTFA...    NaN      기타재난   \n",
       "3  NIA_SL_G4_SAFETYACCIDENTATCHILDRENSAMUSEMENTFA...    NaN      기타재난   \n",
       "4  NIA_SL_G4_SAFETYACCIDENTATCHILDRENSAMUSEMENTFA...    NaN      기타재난   \n",
       "\n",
       "                                         detail  \\\n",
       "0  SAFETYACCIDENTATCHILDRENSAMUSEMENTFACILITIES   \n",
       "1  SAFETYACCIDENTATCHILDRENSAMUSEMENTFACILITIES   \n",
       "2  SAFETYACCIDENTATCHILDRENSAMUSEMENTFACILITIES   \n",
       "3  SAFETYACCIDENTATCHILDRENSAMUSEMENTFACILITIES   \n",
       "4  SAFETYACCIDENTATCHILDRENSAMUSEMENTFACILITIES   \n",
       "\n",
       "                                               gloss  \\\n",
       "0  회사1|다스리다1|편하다1|단체1|날짜:8월|아이1|편하다1|실수1|사고1|계산1|...   \n",
       "1             회사1|편하다1|날짜:0월5일|편하다1|사고1|그네2|놀다1|아니다1   \n",
       "2  회사1|다스리다1|편하다1|단체1|지난1|날1|아이1|편하다1|실수1|사고1|계산1...   \n",
       "3  회사1|다스리다1|편하다1|단체1|날짜:4월|아이1|편하다1|실수1|사고1|계산1|...   \n",
       "4  회사1|다스리다1|편하다1|단체1|아이1|편하다1|실수1|사고1|계산1|놀다1|때1...   \n",
       "\n",
       "                                                text  \n",
       "0  [행정안전부] 8월 4주 어린이 안전사고 14건 발생. 시소는 서로 마주보고 앉아서...  \n",
       "1  [행정안전부] 최근 5일 어린이 안전사고 15건 발생. 그네를 이용할 때 움직이는 ...  \n",
       "2  [행정안전부] 최근 15일 어린이 안전사고 11건 발생. 건너는 기구를 이용할 때,...  \n",
       "3  [행정안전부] 4월 어린이 안전사고 12건 발생. 놀이시설에서는 학용품 및 공 등 ...  \n",
       "4  [행정안전부] 어제 어린이 안전사고 7건 발생. 물이용 놀이기구를 이용할 때, 놀이...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv('./gloss_info_JM.csv')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g_type \u001b[38;5;129;01min\u001b[39;00m gloss_type:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v_point \u001b[38;5;129;01min\u001b[39;00m view_point:\n\u001b[0;32m---> 27\u001b[0m         new_data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(file_name)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mglob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m004.수어영상/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mphase_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/라벨링데이터/REAL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mg_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/*/*/*_*_*_*_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mv_point\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_list \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     data_list \u001b[38;5;241m=\u001b[39m new_data\n",
      "File \u001b[0;32m~/.conda/envs/signlang/lib/python3.9/glob.py:22\u001b[0m, in \u001b[0;36mglob\u001b[0;34m(pathname, recursive)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mglob\u001b[39m(pathname, \u001b[38;5;241m*\u001b[39m, recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a list of paths matching a pathname pattern.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    The pattern may contain simple shell-style wildcards a la\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    zero or more directories and subdirectories.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miglob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpathname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/signlang/lib/python3.9/glob.py:75\u001b[0m, in \u001b[0;36m_iglob\u001b[0;34m(pathname, recursive, dironly)\u001b[0m\n\u001b[1;32m     73\u001b[0m     glob_in_dir \u001b[38;5;241m=\u001b[39m _glob0\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dirname \u001b[38;5;129;01min\u001b[39;00m dirs:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mglob_in_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbasename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdironly\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirname, name)\n",
      "File \u001b[0;32m~/.conda/envs/signlang/lib/python3.9/glob.py:86\u001b[0m, in \u001b[0;36m_glob1\u001b[0;34m(dirname, pattern, dironly)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _ishidden(pattern):\n\u001b[1;32m     85\u001b[0m     names \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m names \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _ishidden(x))\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfnmatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/signlang/lib/python3.9/fnmatch.py:61\u001b[0m, in \u001b[0;36mfilter\u001b[0;34m(names, pat)\u001b[0m\n\u001b[1;32m     58\u001b[0m match \u001b[38;5;241m=\u001b[39m _compile_pattern(pat)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;129;01mis\u001b[39;00m posixpath:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# normcase on posix is NOP. Optimize it away from the loop.\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m names:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m match(name):\n\u001b[1;32m     63\u001b[0m             result\u001b[38;5;241m.\u001b[39mappend(name)\n",
      "File \u001b[0;32m~/.conda/envs/signlang/lib/python3.9/glob.py:85\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     83\u001b[0m names \u001b[38;5;241m=\u001b[39m _listdir(dirname, dironly)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _ishidden(pattern):\n\u001b[0;32m---> 85\u001b[0m     names \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m names \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43m_ishidden\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fnmatch\u001b[38;5;241m.\u001b[39mfilter(names, pattern)\n",
      "File \u001b[0;32m~/.conda/envs/signlang/lib/python3.9/glob.py:159\u001b[0m, in \u001b[0;36m_ishidden\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    156\u001b[0m         match \u001b[38;5;241m=\u001b[39m magic_check\u001b[38;5;241m.\u001b[39msearch(s)\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m match \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ishidden\u001b[39m(path):\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m path[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_isrecursive\u001b[39m(pattern):\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1758\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.ThreadTracer.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/signlang/lib/python3.9/site-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_is_thread_alive.py:9\u001b[0m, in \u001b[0;36mis_thread_alive\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m      6\u001b[0m _temp \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mThread()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_temp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_stopped\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# Python 3.x has this\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_thread_alive\u001b[39m(t):\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_temp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_Thread__stopped\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# Python 2.x has this\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "base_path = '/home/horang1804/HDD1/dataset/aihub_sign'\n",
    "gloss_type = ['WORD','SEN']\n",
    "view_point = ['F']\n",
    "phase = 'train'\n",
    "if phase=='train':\n",
    "    phase_dir = '1.Training'\n",
    "elif phase=='valid':\n",
    "    phase_dir='2.Validation'\n",
    "\n",
    "with open('./word2id.json', \"r\") as st_json: word2id = json.load(st_json)\n",
    "with open('./id2word.json', \"r\") as st_json: id2word = json.load(st_json)\n",
    "\n",
    "token_len = len(id2word)\n",
    "\n",
    "data_list = None \n",
    "\n",
    "new_data = []\n",
    "for g_type in gloss_type:\n",
    "    for v_point in view_point:\n",
    "        new_data += [os.path.split(file_name)[-1] for file_name in glob.glob(os.path.join(base_path,f'004.수어영상/{phase_dir}/라벨링데이터/REAL',f'{g_type}/*/*/*_*_*_*_{v_point}'))]\n",
    "if data_list is None:\n",
    "    data_list = new_data\n",
    "else:\n",
    "    data_list = list(set(data_list).intersection(new_data))\n",
    "\n",
    "\n",
    "\n",
    "new_data = []\n",
    "for g_type in gloss_type:\n",
    "    for v_point in view_point:\n",
    "        new_data += [os.path.split(file_name)[-1].split('_morpheme')[0] for file_name in glob.glob(os.path.join(base_path,f'004.수어영상/{phase_dir}/라벨링데이터/REAL',f'{g_type}/morpheme/*/*_*_*_*_{v_point}_*'))]\n",
    "if data_list is None:\n",
    "    data_list = new_data\n",
    "else:\n",
    "    data_list = list(set(data_list).intersection(new_data))\n",
    "\n",
    "df = pd.DataFrame(columns=['file_id','gloss', 'start', 'end'])\n",
    "idx_df = 0\n",
    "for file_id in tqdm(data_list):\n",
    "    _,_,gtid,ctid,view_point = file_id.split('_')\n",
    "\n",
    "    gloss_type, data_id = gtid[:-4],gtid[-4:]\n",
    "    collect_type, dir_id = ctid[:-2],ctid[-2:]\n",
    "\n",
    "    pose2d = None\n",
    "    flist = glob.glob(os.path.join(base_path,f'004.수어영상/{phase_dir}/라벨링데이터/REAL',f'{gloss_type}/keypoint/{dir_id}/{file_id}/*'))\n",
    "    # flist = glob.glob(os.path.join(self.base_path,'004.수어영상/1.Training/라벨링데이터/REAL',f'{gloss_type}/{dir_id}_{collect_type.lower()}_{gloss_type.lower()}_keypoint/{dir_id}/{file_id}/*'))\n",
    "    keypoints_info = defaultdict(list)\n",
    "    for fname in flist:\n",
    "        with open(fname, \"r\") as st_json: keypoint_json = json.load(st_json)\n",
    "        for k, v in keypoint_json['people'].items():\n",
    "            if k[-2:]=='2d':\n",
    "                keypoints_info[k].append(np.array(v).reshape(-1,3))\n",
    "\n",
    "    pose2d = np.stack(keypoints_info['pose_keypoints_2d']) # seq, 25, 3\n",
    "    handleft2d = np.stack(keypoints_info['hand_left_keypoints_2d']) # seq, 21, 3\n",
    "    handright2d = np.stack(keypoints_info['hand_right_keypoints_2d']) # seq, 21 ,3\n",
    "    face2d = np.stack(keypoints_info['face_keypoints_2d']) # seq, 70, 3\n",
    "    pose = np.concatenate([pose2d[:,:,:2], handleft2d[:,:,:2], handright2d[:,:,:2], face2d[:,:,:2]],1)\n",
    "    # pose = pose.astype(np.float32)\n",
    "    seq_len = pose2d.shape[0]\n",
    "\n",
    "    # gloss_seq = None\n",
    "    # gloss_seq = np.ones(seq_len)*word2id['<sos>']\n",
    "    fname = os.path.join(base_path,f'004.수어영상/{phase_dir}/라벨링데이터/REAL',f'{gloss_type}/morpheme/{dir_id}/{file_id}_morpheme.json')\n",
    "    with open(fname, \"r\") as st_json: gloss_json = json.load(st_json)\n",
    "    gloss_info = defaultdict(list)\n",
    "    # gloss_info['start_time'].append(0)\n",
    "    # gloss_info['gloss'].append('<sos>')\n",
    "    # start_time=0\n",
    "    # end_time=0\n",
    "    # start_idx=0\n",
    "    # end_idx=-1\n",
    "    for d in gloss_json['data']:\n",
    "        start_time = d['start']\n",
    "        end_time = d['end']\n",
    "        # start_idx = int(start_time*30)\n",
    "        # end_idx = int(end_time*30)\n",
    "        gloss = d['attributes'][0]['name']\n",
    "        gloss_info['start_time'].append(start_time)\n",
    "        gloss_info['end_time'].append(end_time)\n",
    "        gloss_info['gloss'].append(gloss)\n",
    "        # gloss_seq[start_idx:end_idx]=word2id[gloss]\n",
    "    # gloss_info['end_time'].append(end_time) #real end 계산필요\n",
    "    # gloss_info['gloss'].append('<eos>')\n",
    "    # gloss_seq[end_idx:]=word2id['<eos>']\n",
    "    # gloss_info['gloss_id'] = np.array([word2id[g] for g in gloss_info['gloss']])\n",
    "\n",
    "    pose2d\n",
    "    df.loc[idx_df] = [file_id, \n",
    "                      '|'.join(gloss_info['gloss']), \n",
    "                      '|'.join(gloss_info['start_time']), \n",
    "                      '|'.join(gloss_info['end_time'])]\n",
    "    idx_df +=1\n",
    "    # np.save(os.path.join(base_path,'pose','valid',collect_type,gloss_type,dir_id,f'{file_id}.npy'),pose)\n",
    "    # break\n",
    "\n",
    "df.to_csv(f'./gloss_label_{phase}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signlang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
